{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence_classification_iitpatna_komal.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJyPpaNv0/mrFlL01bJdqk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amanpatni211/Cat-or-No-Cat/blob/main/Sentence_classification_iitpatna_komal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhpgQpSwGeXq",
        "outputId": "79e0c65d-36d9-4b8c-95b8-02dab95e45f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ncg-task/training-data.git \"/content/train\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAfF7fE6HRKD",
        "outputId": "07651727-b78d-4a97-f53e-4439e3604306"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/train'...\n",
            "remote: Enumerating objects: 6864, done.\u001b[K\n",
            "remote: Counting objects: 100% (3083/3083), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2728/2728), done.\u001b[K\n",
            "remote: Total 6864 (delta 567), reused 2504 (delta 279), pack-reused 3781\u001b[K\n",
            "Receiving objects: 100% (6864/6864), 157.36 MiB | 32.19 MiB/s, done.\n",
            "Resolving deltas: 100% (660/660), done.\n",
            "Checking out files: 100% (3286/3286), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Loading Training Data and store it\n",
        "input_dir = \"/content/train/\"\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "list_of_folders = [\"query_wellformedness\", \"passage_re-ranking\", \"part-of-speech_tagging\", \n",
        "         \"sentence_compression\", \"sentiment_analysis\", \"temporal_information_extraction\", \n",
        "         \"phrase_grounding\", \"text_generation\", \"text-to-speech_synthesis\", \n",
        "         \"smile_recognition\", \"topic_models\", \"question_generation\", \n",
        "         \"relation_extraction\", \"paraphrase_generation\", \"question_similarity\", \n",
        "         \"question_answering\", \"sentence_classification\", \"prosody_prediction\", \n",
        "         \"semantic_role_labeling\", \"text_summarization\", \"semantic_parsing\",\n",
        "         \"sarcasm_detection\", \"natural_language_inference\", \"negation_scope_resolution\"]\n",
        "input_stanza_list = []\n",
        "input_stanza_len = []\n",
        "input_sent_num_list = []\n",
        "file_name_list = []\n",
        "for fls in list_of_folders:\n",
        "  count=0\n",
        "  for i in os.listdir(input_dir + fls + '/'):\n",
        "    count=count+1\n",
        "    for files in os.listdir(input_dir + fls + '/' + str(i)):\n",
        "      if files.endswith(\"Stanza-out.txt\"):\n",
        "        stanza_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
        "        stanza_lines = stanza_file.read()\n",
        "        stanza_lines_list = list(filter(None,map(lambda x:x.lower(),stanza_lines.splitlines()))) # filter empty strings and split into lines\n",
        "        input_stanza_len.append(len(stanza_lines_list))\n",
        "        input_stanza_list.append(stanza_lines_list)\n",
        "      if files.endswith(\"sentences.txt\"):\n",
        "        sentence_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
        "        sentence_num_list = list(filter(None,sentence_file.read().splitlines())) # filter empty strings and split into lines\n",
        "        input_sent_num_list.append(sentence_num_list)\n",
        "    file_name_list.append(fls + '/' + str(i))\n",
        "  print(\"Training complete\",fls, count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAL7JAQFHV9j",
        "outputId": "158591d1-84e6-4ee5-b873-4566966e0fbd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete query_wellformedness 1\n",
            "Training complete passage_re-ranking 2\n",
            "Training complete part-of-speech_tagging 8\n",
            "Training complete sentence_compression 4\n",
            "Training complete sentiment_analysis 52\n",
            "Training complete temporal_information_extraction 2\n",
            "Training complete phrase_grounding 1\n",
            "Training complete text_generation 6\n",
            "Training complete text-to-speech_synthesis 3\n",
            "Training complete smile_recognition 1\n",
            "Training complete topic_models 1\n",
            "Training complete question_generation 2\n",
            "Training complete relation_extraction 14\n",
            "Training complete paraphrase_generation 2\n",
            "Training complete question_similarity 1\n",
            "Training complete question_answering 6\n",
            "Training complete sentence_classification 3\n",
            "Training complete prosody_prediction 1\n",
            "Training complete semantic_role_labeling 5\n",
            "Training complete text_summarization 15\n",
            "Training complete semantic_parsing 3\n",
            "Training complete sarcasm_detection 2\n",
            "Training complete natural_language_inference 101\n",
            "Training complete negation_scope_resolution 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ncg-task/trial-data.git \"/content/valid\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c95rMNg1H1Vo",
        "outputId": "26e344a2-9c6a-452e-d867-3333dea75863"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/valid'...\n",
            "remote: Enumerating objects: 1636, done.\u001b[K\n",
            "remote: Counting objects: 100% (1636/1636), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1438/1438), done.\u001b[K\n",
            "remote: Total 1636 (delta 580), reused 1224 (delta 170), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1636/1636), 27.88 MiB | 22.13 MiB/s, done.\n",
            "Resolving deltas: 100% (580/580), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### Loading valid dataset\n",
        "val_input_dir = \"/content/valid/\"\n",
        "val_list_of_folders = [\"machine-translation\", \"named-entity-recognition\", \"question-answering\",\n",
        "         \"relation-classification\", \"text-classification\"]\n",
        "val_input_stanza_list = []\n",
        "val_input_sent_num_list = []\n",
        "val_file_name_list = []\n",
        "val_input_stanza_len = []\n",
        "for fls in val_list_of_folders:\n",
        "  count=0\n",
        "  for i in os.listdir(val_input_dir + fls + '/'):\n",
        "    count=count+1\n",
        "    for files in os.listdir(val_input_dir + fls + '/' + str(i)):\n",
        "      if files.endswith(\"Stanza-out.txt\"):\n",
        "        stanza_file = open(val_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
        "        stanza_lines = stanza_file.read()\n",
        "        stanza_lines_list = list(filter(None, map(lambda x:x.lower(),stanza_lines.splitlines()))) # filter empty strings and split into lines\n",
        "        val_input_stanza_len.append(len(stanza_lines_list))\n",
        "        val_input_stanza_list.append(stanza_lines_list)\n",
        "      if files.endswith(\"sentences.txt\"):\n",
        "        sentence_file = open(val_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
        "        sentence_num_list = list(filter(None,sentence_file.read().splitlines())) # filter empty strings and split into lines\n",
        "        val_input_sent_num_list.append(sentence_num_list)\n",
        "    val_file_name_list.append(fls + '/' + str(i))\n",
        "  print(\"Valid complete\",fls,count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM3F5icKHYUV",
        "outputId": "3764f2db-27f0-4d1a-c6e3-1ad5771fe0bf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid complete machine-translation 10\n",
            "Valid complete named-entity-recognition 10\n",
            "Valid complete question-answering 10\n",
            "Valid complete relation-classification 10\n",
            "Valid complete text-classification 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### string to int \n",
        "#### conversion for Train set\n",
        "input_sent_num_list = [[int(s) for s in sublist] for sublist in input_sent_num_list] # convert sentence list string to integer\n",
        "input_sent_num_list = [list(set(x)) for x in input_sent_num_list]\n",
        "\n",
        "#### conversion for Valid set\n",
        "val_input_sent_num_list = [[int(s) for s in sublist] for sublist in val_input_sent_num_list] # convert sentence list string to integer\n",
        "val_input_sent_num_list = [list(set(x)) for x in val_input_sent_num_list]"
      ],
      "metadata": {
        "id": "A89EXvEfHZ5A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Ground truth label formation for classification\n",
        "#### Train set\n",
        "multihot_input_sent = []\n",
        "for i in range(len(input_stanza_list)):\n",
        "  temp =[0]*input_stanza_len[i]\n",
        "  for j in range(len(input_sent_num_list[i])):\n",
        "    t1 = input_sent_num_list[i][j] -1\n",
        "    temp[t1] = 1\n",
        "  multihot_input_sent.append(temp)\n",
        "#### valid set\n",
        "val_multihot_input_sent = []\n",
        "for i in range(len(val_input_stanza_list)):\n",
        "  temp =[0]*val_input_stanza_len[i]\n",
        "  for j in range(len(val_input_sent_num_list[i])):\n",
        "    t1 = val_input_sent_num_list[i][j] -1\n",
        "    temp[t1] = 1\n",
        "  val_multihot_input_sent.append(temp)"
      ],
      "metadata": {
        "id": "1M1SWqsbHbXK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Function for flattening 2d list to 1d list\n",
        "from collections import Iterable\n",
        "def flatten(lis):\n",
        "  for item in lis:\n",
        "    if isinstance(item, Iterable) and not isinstance(item, str):\n",
        "      for x in flatten(item):\n",
        "        yield x\n",
        "    else:        \n",
        "      yield item"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhxYbdxwHdb2",
        "outputId": "ed2691e5-4f82-4010-a0e8-422fb6f69b9e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Flatten the list and use heuristic to remove vague sentences\n",
        "#### Train set\n",
        "from collections import Counter\n",
        "\n",
        "train_sentences = list(flatten(input_stanza_list))\n",
        "train_label = list(flatten(multihot_input_sent))\n",
        "\n",
        "train_tuple = list(set((zip(train_sentences,train_label))))\n",
        "train_in_sentences = []\n",
        "train_sent_label = []\n",
        "for stan,lab in train_tuple:\n",
        "  if len(stan) >4:\n",
        "    train_in_sentences.append(stan)\n",
        "    train_sent_label.append(lab)\n",
        "\n",
        "valid_sentences = list(flatten(val_input_stanza_list))\n",
        "valid_label = list(flatten(val_multihot_input_sent))\n",
        "\n",
        "valid_tuple = list(set((zip(valid_sentences,valid_label))))\n",
        "valid_in_sentences = []\n",
        "valid_sent_label = []\n",
        "for stan,lab in valid_tuple:\n",
        "  if len(stan) >4:\n",
        "    valid_in_sentences.append(stan)\n",
        "    valid_sent_label.append(lab)\n",
        "\n",
        "print(\"train size\",len(train_in_sentences),len(train_sent_label))\n",
        "print(\"valid size\",len(valid_in_sentences),len(valid_sent_label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4_sRcQJHe7S",
        "outputId": "edd2bdbd-aefc-4d1f-a4c1-733fd82ba4df"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train size 51296 51296\n",
            "valid size 10681 10681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Five timesOversampling of minority class in train set\n",
        "train_tuple = list(set((zip(train_in_sentences,train_sent_label))))\n",
        "train_sum_sent = 5 * [stan for stan,label in train_tuple if label==1]\n",
        "train_nonsum_sent = [stan for stan,label in train_tuple if label==0]\n",
        "train_in_sentences = train_sum_sent + train_nonsum_sent\n",
        "train_sent_label = len(train_sum_sent)*[1] + len(train_nonsum_sent)*[0]\n",
        "print(\"Oversampled train size\",len(train_in_sentences),len(train_sent_label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H40b-9sFHgqH",
        "outputId": "ae80864f-e948-4c8b-c1d0-616a909198bd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oversampled train size 71396 71396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL A tensorflow simple dense model\n"
      ],
      "metadata": {
        "id": "VAEAoztoIHj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "Xuq02ybZK2Du"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sentences = train_in_sentences\n",
        "training_labels = train_sent_label\n",
        "testing_sentences = valid_in_sentences\n",
        "testing_labels = valid_sent_label"
      ],
      "metadata": {
        "id": "zRbQnqd8H7bi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(training_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1I3ppC3KwWq",
        "outputId": "3cdf1c68-a6e7-479e-cb12-1e0e36c4dc20"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71396"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 16\n",
        "max_length = 10000000\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "training_size = 71396"
      ],
      "metadata": {
        "id": "3vBPmLWsLTAO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "metadata": {
        "id": "sgjD8NO1KpXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uiCCxGPZKzxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Need this block to get it to work with TensorFlow 2.x\n",
        "import numpy as np\n",
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_labels)\n",
        "testing_padded = np.array(testing_padded)\n",
        "testing_labels = np.array(testing_labels)"
      ],
      "metadata": {
        "id": "GPglG9g4Le7D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}